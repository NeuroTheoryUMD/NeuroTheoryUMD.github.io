<!doctype html>
<html>
<head>
	<meta charset="UTF-8">
	<title>NeuroTheory Lab Research</title>
	<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700">
	<link rel="stylesheet" href="../NTlab_styles.css">
</head>
	
<body>
	<!-- Header -->
	<header>
		<a class="site-logo" href="index.html">
			<img src="../Images/NTlabBackStrip.jpg" width="100%">
			<div class="NTlab"><h1>NeuroTheory Lab Research</h1></div>

			<nav class="site-nav">
				<ul class="menu">
					<li><a class="menu_alt" href="../code.html"><h3>Code</h3></a></li>
					<li><a class="menu_alt" href="../presentations.html"><h3>Presentations</h3></a></li>
					<li><a class="menu_alt" href="../publications.html"><h3>Publications</h3></a></li>
					<li><a class="menu_alt" href="../people.html"><h3>People</h3></a></li>
					<li><a class="menu_alt" href="../research.html"><h3><span style="color:#060265"><u>Research</u></span></h3></a></li>
				</ul>
			</nav>
		</a>
	</header>
	
	<!-- Main content -->
	<main>
		<section class="intro">
			<h1>Cortical processing of vision</h1>
			<h3>Color vision</h3>
			<p>Color and form are often treated as separable features of an image. One can recognize shapes in achromatic photographs and conceptualize the color of an object abstracted from shape. Yet color-specific processing is embedded throughout the visual pathway from the first stage of the visual pathway, where three different types of light sensors (“cones”) with sensitivity to different parts of the visual spectrum initially convert light into electrical impulses. The color of a given point can in principle be determined by comparing the activation of the three different cone types, but the separate color channels are maintained until the primary visual cortex (V1), where they are finally combined in neurons that concurrently have sensitivity to different spatial patterns. Indeed, while it was initially thought that color and form were processed through separate pathways within V1, recent experiments have highlighted that a surprising fraction of V1 neurons mixes them together in a diversity of ways. Exactly how the mixing occurs, and for what purpose, are critical open questions in understanding human vision, and have been difficult to answer because such mixing is too complicated to characterize using traditional approaches. This project will combine large-scale recording of V1 neural activity during tailored “spatio-chromatic” visual stimulation with new computational approaches, which will both offer an unprecedented high-resolution description of color processing within V1, while determining the underlying function of spatio-chromatic mixing in supporting natural color vision.	
			</p>
			<p><i>Experimental collaborator</i>: <a href="https://www.nei.nih.gov/research/research-labs-and-branches/we-are-nei-intramural/bevil-conway" target="_new">Bevil Conway</a> (NEI)</p>
			<ul>
				<li>Recent <a href="../PresentationFiles/VSS2022_Bartsch_poster.pdf" target="_new">poster</a> presenting preliminary work at the Visual Sciences Society (VSS) 2022 Conference (St. Petersberg, FL)</p></li>
			</ul>
			
			<h3>Binocular integration and disparity selectivity</h3>
			<p>Despite having two distinct visual sensors — the eyes — visual perception usually consists of a single fused image. This requires the visual system to combine disparate “monocular” information from each retina into a “binocular” representation: a process thought to largely occur in the primary visual cortex (V1), whose inputs from the LGN are monocular, and its outputs are largely binocular. Combining information from both eyes is not trivial, because objects at different depths will have binocular disparity, that is, they will have a shifted position in both eyes. The visual system must therefore take disparity into account when combining information from each eye, and indeed V1 has a large number of disparity-tuned neurons. However, despite a number of conceptual models for how disparity processing *should* occur, no model thus far has been able to adequately reproduce the disparity tuning of V1 neurons using modern physiological approaches.</p>

			<p><i>Experimental collaborator</i>: <a href="https://www.nei.nih.gov/research/research-labs-and-branches/we-are-nei-intramural/bruce-cumming" target="_new">Bruce Cumming</a> (NEI)</p>
			<ul>
				<li><b>[SFN 2021]</b> Binocular integration as nonlinear mixing: how binocular neurons in primary visual cortex preserve eye-specific information for downstream visual processing, <i>presented by Ethan Cheng</i> [<a href="../PresentationFiles/SFN2021_Cheng_slides.pdf" target="_new">Slides</a>] [<a href="../PresentationFiles/SFN2021_Cheng_video.mp4" target="_new">Video summary</a>]</li>
				<li><b>[SFN 2021]</b> Amplification of disparity selectivity by spatial convolutions in the primary visual cortex, <i>presented by Felix Bartsch</i> [<a href="../PresentationFiles/SFN2021_Bartsch_slides.pdf" target="_new">Slides</a>] [<a href="../PresentationFiles/SFN2021_Bartsch_video.mp4" target="_new">Video summary</a>]</li>
				<li>Henriksen S, <b>Butts DA</b>, Read JCA, Cumming BG (2018) Current models cannot account for V1's specialisation for binocular natural image statistics. <i>bioRxiv</i>: 497008.  [<a href="https://www.biorxiv.org/content/10.1101/497008v1" target="_new">Preprint</a>]</li>

			</ul>
			<h3>Statistical models of visual neurons (general)</h3>
			<p>With modern neurophysiological methods able to record neural activity throughout the visual pathway in the context of arbitrarily complex visual stimulation, our understanding of visual system function is becoming lim- ited by the available models of visual neurons that can be directly related to such data. Different forms of statistical models are now being used to probe the cellular and circuit mechanisms shaping neural activity, understand how neural selectivity to complex visual features is computed, and derive the ways in which neurons contribute to systems-level visual processing. However, models that are able to more accurately reproduce observed neural activity often defy simple interpretations. As a result, rather than being used solely to connect with existing theories of visual processing, statistical modeling will increasingly drive the evolution of more sophisticated theories.</p>
			<ul>
				<li><b>Butts DA</b> (2019) Data-driven approaches to understanding visual neuron activity. <i>Annual Review of Vision Science</i> <b>5</b>: 451-77. [<a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-vision-091718-014731" target="_new">Link to journal</a>, <a href="Publications_files/ARVSpreprint.pdf">Preprint</a>]</li>
				<li><b>Bartsch F</b>, Cumming BG, <b>Butts DA</b> (2021) Model-based characterization of the selectivity of neurons in primary visual cortex. <i>bioRxiv</i> <a href="https://www.biorxiv.org/content/10.1101/2021.09.13.460153v1" target="_new">2021.09.13.460153</a></li>
				<li>Shi Q, <b>Gupta P</b>, <b>Boukhvalova A</b>, Singer JH, <b>Butts DA</b> (2019) Functional characterization of retinal ganglion cells using tailored nonlinear modeling. <i>Scientific Reports</i> <b>9</b>: 8713. [<a href="https://www.nature.com/articles/s41598-019-45048-8" target="_new">PDF</a>, <a href="#">Code</a>]</li>
				<li><b>Butts DA</b>, <b>Cui Y</b>, Casti ARR (2016) Nonlinear computation shaping temporal processing of pre-cortical vision. <i>Journal of Neurophysiology</i> <b>116</b>: 1344-57. [<a href="http://jn.physiology.org/content/116/3/1344" target="_new">Journal website</a>]</li>
				<li><b>Cui Y</b>, Liu L, Khawaja FA, Pack CC, <b>Butts DA</b> (2013) Diverse suppressive influences in area MT and selectivity to complex motion features. <i>Journal of Neuroscience</i> <b>33</b>: 16715-28. [<a href="Publications_files/Cui2013.pdf">PDF</a>]</li>
				<li><b>McFarland JM</b>, <b>Cui Y</b>, <b>Butts DA</b> (2013) Inferring nonlinear neuronal computation based on physiologically plausible inputs. <i>PLoS Computational Biology</i> <b>9</b>: e1003143. [<a href="http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003143" target="_new">PDF</a>] [<a href="#">Code</a>]</li>
			</ul>
		</section>
			</main>
</body>
</html>
